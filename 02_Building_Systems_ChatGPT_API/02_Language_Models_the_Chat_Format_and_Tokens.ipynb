{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d00ca8",
   "metadata": {},
   "source": [
    "# L1 Language Models, the Chat Format and Tokens\n",
    "\n",
    "I'd like to share with you \n",
    "an overview of how LLMs, Large Language Models, work. \n",
    "We'll go into how they are trained, as well as details like\n",
    "- the tokenizer and \n",
    "- how that can affect the output of when you prompt an LLM. \n",
    "\n",
    "And we'll also take a look at the chat format for LLMs, which \n",
    "is a way of specifying both system as \n",
    "well as user messages and understand what you \n",
    "can do with that capability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a82827",
   "metadata": {},
   "source": [
    "![LLM](immagini\\01_LLM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b97194c",
   "metadata": {},
   "source": [
    "The main tool used to train an LLM is actually supervised learning. \n",
    "\n",
    "In supervised learning, a computer learns an input-output \n",
    "or X or Y mapping using labeled training data.  So \n",
    "for example, if you're using supervised learning to \n",
    "learn to classify the sentiment of restaurant reviews, you might \n",
    "collect a training set. (x --> y)\n",
    "\n",
    "The process for supervised learning is \n",
    "typically to get labeled data and then train \n",
    "AI model on data. \n",
    "And after training, you can then deploy \n",
    "and call the model.\n",
    "\n",
    "It turns out that supervised learning is a \n",
    "core building block for training Large Language Models. \n",
    "Specifically, a Large Language Model can be built by using \n",
    "supervised learning to repeatedly predict the next word. \n",
    "\n",
    "Let's say that in your training sets of a lot of text data, you \n",
    "have to sentence, \"My favorite food is a bagel \n",
    "with cream cheese and lox.\". \n",
    "Then this sentence is turned into a sequence of training examples, \n",
    "where given a sentence fragment, \"My favorite food is a\", \n",
    "if you want to predict the next word in this case was \"bagel\", \n",
    "or given the sentence fragment or sentence prefix, \n",
    "\"My favorite food is a bagel\", the next word in this case would be \n",
    "\"with\", and so on. \n",
    "\n",
    "And given a large training set of hundreds of \n",
    "billions or sometimes even more words, you can then create \n",
    "a massive training set where you can start \n",
    "off with part of a sentence or part of a piece of \n",
    "text and repeatedly ask the language model to \n",
    "learn to predict what is the next word. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db39bfb1",
   "metadata": {},
   "source": [
    "![supervised_learning_llm](immagini\\02_supervised_learning_llm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4e7e6",
   "metadata": {},
   "source": [
    "So today there are broadly two major types \n",
    "of Large Language Models. The first is a \"Base LLM\" \n",
    "and the second, which is what is increasingly used, \n",
    "is the \"Instruction Tuned LLM\". \n",
    "\n",
    "#### - BASE LLM\n",
    "So the base LLM repeatedly predicts the next \n",
    "word based on text training data.\n",
    "#### - INSTRUCTION TUNED LLM\n",
    "An Instruction Tuned LLM instead tries to follow \n",
    "instructions.\n",
    "\n",
    "<span style=\"color:blue\">How do you go from a Base LLM to an Instruction Tuned LLM?</span> \n",
    "\n",
    "This is what the process of training an Instruction Tuned LLM, \n",
    "like ChatGPT, looks like. \n",
    "\n",
    "You first train a Base LLM on a lot of data, \n",
    "so hundreds of billions of words, maybe even more. And this is a \n",
    "process that can take months on a large \n",
    "supercomputing system. \n",
    "After you've trained the Base LLM, you would then further train \n",
    "the model by fine-tuning it on a smaller set of examples, where the \n",
    "output follows an input instruction.\n",
    "\n",
    "And so, for example, you may \n",
    "have **contractors** help you write a lot of examples of an instruction, \n",
    "and then a good response to an instruction. \n",
    "And that creates a training set to carry \n",
    "out this additional fine-tuning. So that learns to predict what is \n",
    "the next word if it's trying to follow an instruction.\n",
    "\n",
    "After that, \n",
    "to improve the **quality** of the LLM's output, a \n",
    "common process now is to obtain human ratings of the quality of many different \n",
    "LLM outputs on criteria, such as whether \n",
    "the output is **helpful, honest, and harmless.**\n",
    "\n",
    "And you can then further tune the LLM \n",
    "to increase the probability of its **generating the \n",
    "more highly rated outputs**. And the most common technique \n",
    "to do this is RLHF, which stands for Reinforcement Learning from \n",
    "Human Feedback.\n",
    "\n",
    "And whereas training the \n",
    "Base LLM can take months, the process of going \n",
    "from the Base LLM to the Instruction Tuned \n",
    "LLM can be done in maybe days on a much more modest size data sets, \n",
    "and much more modest size computational resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76ebe33",
   "metadata": {},
   "source": [
    "![base_instruction_tuned_llm](immagini\\03_base_instruction_tuned_llm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f2070d",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### Load the API key and relevant Python libaries.\n",
    "In this course, we've provided some code that loads the OpenAI API key for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e62407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff3621",
   "metadata": {},
   "source": [
    "#### helper function\n",
    "This may look familiar if you took the earlier course \"ChatGPT Prompt Engineering for Developers\" Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d240cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "# helper function to get a completion given a prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d697fd8",
   "metadata": {},
   "source": [
    "## Prompt the model and get a completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aac4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6894d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6553ff",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "The capital of France is Paris."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4153ea15",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(\"Take the letters in lollipop \\\n",
    "and reverse them\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8b78d6",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "The reversed letters of \"lollipop\" are \"pillipol\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9023ae",
   "metadata": {},
   "source": [
    "\"lollipop\" in reverse should be \"popillol\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b062618",
   "metadata": {},
   "source": [
    "**Take the letters in ... and reverse them. This seems like an easy task.\n",
    "So why is ChatGPT unable to do what seems like a relatively simple task? \n",
    "It turns out that there's one more important detail for how a Large Language Model works, which is it doesn't actually epeatedly predict the next word, it instead repeatedly predicts the NEXT TOKEN.**\n",
    "\n",
    "What an LLM actually does is it will take a sequence of characters and group the \n",
    "characters together to form tokens that \n",
    "comprise commonly occurring sequences of characters.\n",
    "\n",
    "And because ChatGPT isn't seeing the individual letters, is \n",
    "instead seeing these three tokens, it's more difficult for it to \n",
    "correctly print out these letters in reverse order. \n",
    " \n",
    "So here's a trick you can use to fix this. \n",
    "If I were to add dashes to the word dashes, \n",
    "between these letters, and spaces would work too, \n",
    "or other things would work too, and tell it to take the letters and \n",
    "lollipop and reverse them, then it actually does \n",
    "a much better job, this L-O-L-L-I-P-O-P. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(\"\"\"Take the letters in \\\n",
    "l-o-l-l-i-p-o-p and reverse them\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8af910",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19cba5d",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "'p-o-p-i-l-l-o-l'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef25db69",
   "metadata": {},
   "source": [
    "![Tokens](immagini\\04_token.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2695e747",
   "metadata": {},
   "source": [
    "For the English language, one token roughly on average, \n",
    "corresponds to about four characters or about three \n",
    "quarters of a word. \n",
    "And so different Large Language Models \n",
    "will often have different limits on the number of input plus output \n",
    "tokens it can accept.\n",
    "\n",
    "## - INPUT = the message is called \"CONTEXT\"\n",
    "## - OUTPUT = the message is called \"COMPLETION\"\n",
    "\n",
    "The model GPT 3.5 Turbo, for example, the \n",
    "most commonly used chat GPT model, has a limit of roughly **4,000 tokens \n",
    "in the INPUT + OUTPUT.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cd2597",
   "metadata": {},
   "source": [
    "## Helper function (chat format)\n",
    "Here's the helper function we'll use in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a415172",
   "metadata": {},
   "source": [
    "Another powerful way to use an LLM API. \n",
    "Which involves specifying separate:\n",
    "- system, \n",
    "- user and\n",
    "- assistant messages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6826455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0, \n",
    "                                 max_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        # this is the degree of randomness of the model's output\n",
    "        max_tokens=max_tokens, # the maximum number of tokens the model can output \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e06559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':\"\"\"You are an assistant who\\\n",
    " responds in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user', \n",
    " 'content':\"\"\"write me a very short poem\\\n",
    " about a happy carrot\"\"\"},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec32b1a0",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "In the garden it sat, oh so bright,\n",
    "A carrot that brought pure delight.\n",
    "Its vibrant orange, oh so cheery,\n",
    "Bringing joy to all, oh how merry!\n",
    "\n",
    "With a crisp crunch, a happy sound,\n",
    "This carrot's deliciousness did astound.\n",
    "From soil to table, a journey complete,\n",
    "A happy carrot, oh so sweet!\n",
    "\n",
    "It brought smiles to children, big and small,\n",
    "As they munched and chomped, having a ball.\n",
    "A happy carrot, full of glee,\n",
    "Bringing happiness, for all to see!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3262ac4",
   "metadata": {},
   "source": [
    "# CHAT FORMAT:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21cce04",
   "metadata": {},
   "source": [
    "### - The system message specifies the overall tone of what you want the Large Language Model to do.\n",
    "### - The user message is a specific instruction that you wanted to carry out given this higher level behavior that was specified in the system message. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8f3142",
   "metadata": {},
   "source": [
    "#### It will then output an appropriate response following what you asked for in the USER MESSAGE and consistent with the overall behavior set in the SYSTEM MESSAGE. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc40ac3",
   "metadata": {},
   "source": [
    "![Token](immagini\\05_message_chatbot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3325f1",
   "metadata": {},
   "source": [
    "If you want to set the tone, to tell it to have \n",
    "a one sentence long output, then in the system message, \n",
    "I can say all your responses must be one sentence long. \n",
    " \n",
    "And when I execute this, it outputs a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2edb2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'All your responses must be \\\n",
    "one sentence long.'},    \n",
    "{'role':'user',\n",
    " 'content':'write me a story about a happy carrot'},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a458a58",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "Once upon a time, there was a plump, cheerful carrot named Carl who lived in a vibrant vegetable garden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':\"\"\"You are an assistant who \\\n",
    "responds in the style of Dr Seuss. \\\n",
    "All your responses must be one sentence long.\"\"\"},    \n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a story about a happy carrot\"\"\"},\n",
    "] \n",
    "response = get_completion_from_messages(messages, \n",
    "                                        temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c8df4",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "Once upon a time, there was a carrot named Larry who was always merry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4361bacf",
   "metadata": {},
   "source": [
    "If you are using an \n",
    "LLM and you want to know how many tokens are you using, \n",
    "here's a helper function that is a little bit \n",
    "more sophisticated in that it gets a response \n",
    "from the OpenAI API endpoint and then it \n",
    "uses other values in the response to tell \n",
    "you how many prompt tokens, completion tokens, and \n",
    "total tokens were used in your API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be400abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_and_token_count(messages, \n",
    "                                   model=\"gpt-3.5-turbo\", \n",
    "                                   temperature=0, \n",
    "                                   max_tokens=500):\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    \n",
    "    content = response.choices[0].message[\"content\"]\n",
    "    \n",
    "    token_dict = {\n",
    "'prompt_tokens':response['usage']['prompt_tokens'],\n",
    "'completion_tokens':response['usage']['completion_tokens'],\n",
    "'total_tokens':response['usage']['total_tokens'],\n",
    "    }\n",
    "\n",
    "    return content, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who responds\\\n",
    " in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a very short poem \\ \n",
    " about a happy carrot\"\"\"},  \n",
    "] \n",
    "response, token_dict = get_completion_and_token_count(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef1df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69829a69",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "Oh, the happy carrot, so bright and orange,\n",
    "Grown in the garden, a joyful forage.\n",
    "With a smile so wide, from top to bottom,\n",
    "It brings happiness, oh how it blossoms!\n",
    "\n",
    "In the soil it grew, with love and care,\n",
    "Nourished by sunshine, fresh air to share.\n",
    "Its leaves so green, reaching up so high,\n",
    "A happy carrot, oh my, oh my!\n",
    "\n",
    "With a crunch and a munch, it's oh so tasty,\n",
    "Filled with vitamins, oh so hasty.\n",
    "A happy carrot, a delight to eat,\n",
    "Bringing joy and health, oh what a treat!\n",
    "\n",
    "So let's celebrate this veggie so grand,\n",
    "With a happy carrot in each hand.\n",
    "For in its presence, we surely find,\n",
    "A taste of happiness, one of a kind!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f5d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3b380",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "```json\n",
    "{'prompt_tokens': 37, 'completion_tokens': 164, 'total_tokens': 201}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db4f012",
   "metadata": {},
   "source": [
    "#### Notes on using the OpenAI API outside of this classroom\n",
    "\n",
    "To install the OpenAI Python library:\n",
    "```\n",
    "!pip install openai\n",
    "```\n",
    "\n",
    "The library needs to be configured with your account's secret key, which is available on the [website](https://platform.openai.com/account/api-keys). \n",
    "\n",
    "You can either set it as the `OPENAI_API_KEY` environment variable before using the library:\n",
    " ```\n",
    " !export OPENAI_API_KEY='sk-...'\n",
    " ```\n",
    "\n",
    "Or, set `openai.api_key` to its value:\n",
    "\n",
    "```\n",
    "import openai\n",
    "openai.api_key = \"sk-...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472f3377",
   "metadata": {},
   "source": [
    "Now, I want to share with you one more tip for \n",
    "how to use a Large Language Model. \n",
    "Commonly the OpenAI API requires using an API key that's \n",
    "tied to either a free or a paid account. \n",
    "And so many developers will write the API \n",
    "key in plain text like this into their \n",
    "Jupyter notebook. \n",
    "And this is a less secure way of using API keys that \n",
    "I would not recommend you use, because it's just too easy to \n",
    "share this notebook with someone else or check \n",
    "this into GitHub or something and thus end \n",
    "up leaking your API key to someone else. \n",
    "In contrast, what you saw me do in the Jupyter \n",
    "notebook was this piece of code, where I use a library \"dotenv\", \n",
    "and then run this command \"load_dotenv\", \"find_dotenv\" to read \n",
    "a local file which is called \".env\" that contains my secret key. \n",
    "And so with this code snippet, I have locally stored a file called \n",
    "\".env\" that contains my API key. \n",
    "And this loads it into the operating systems environmental \n",
    "variable. \n",
    "And then \"os.getenv, ('OPENAI_API_KEY')\" stores it into this variable. And in this \n",
    "whole process, I don't ever have to enter the API key in plain text and \n",
    "unencrypted plain text into my Jupyter notebook. \n",
    " \n",
    "So this is a relatively more secure and a better \n",
    "way to access the API key. And in fact, this is a general \n",
    "method for storing different API keys from lots \n",
    "of different online services that you might want to use and call \n",
    "from your Jupyter notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a5b21",
   "metadata": {},
   "source": [
    "![Token](immagini\\06_api.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbfa35b",
   "metadata": {},
   "source": [
    "#### A note about the backslash\n",
    "- In the course, we are using a backslash `\\` to make the text fit on the screen without inserting newline '\\n' characters.\n",
    "- GPT-3 isn't really affected whether you insert newline characters or not.  But when working with LLMs in general, you may consider whether newline characters in your prompt may affect the model's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
