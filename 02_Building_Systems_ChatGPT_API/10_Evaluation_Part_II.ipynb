{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb0b5cb",
   "metadata": {},
   "source": [
    "# L9: Evaluation Part II\n",
    "\n",
    "Evaluate LLM responses where there isn't a single \"right answer.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea86d0",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### Load the API key and relevant Python libaries.\n",
    "In this course, we've provided some code that loads the OpenAI API key for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f4987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import utils\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0263c55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=0, max_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens, \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3dc6e4",
   "metadata": {},
   "source": [
    "### Run through the end-to-end system to answer the user query\n",
    "\n",
    "These helper functions are running the chain of promopts that you saw in the earlier videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a77daed",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_msg = f\"\"\"\n",
    "tell me about the smartx pro phone and the fotosnap camera, the dslr one.\n",
    "Also, what TVs or TV related products do you have?\"\"\"\n",
    "\n",
    "products_by_category = utils.get_products_from_query(customer_msg)\n",
    "category_and_product_list = utils.read_string_to_list(products_by_category)\n",
    "product_info = utils.get_mentioned_product_info(category_and_product_list)\n",
    "assistant_answer = utils.answer_user_msg(user_msg=customer_msg,\n",
    "                                                   product_info=product_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98546c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assistant_answer) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac3791b",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "Sure! Let me provide you with some information about the SmartX ProPhone and the FotoSnap DSLR Camera.\n",
    "\n",
    "The SmartX ProPhone is a powerful smartphone with advanced camera features. It has a 6.1-inch display, 128GB storage, a 12MP dual camera, and supports 5G connectivity. The SmartX ProPhone is priced at $899.99 and comes with a 1-year warranty.\n",
    "\n",
    "The FotoSnap DSLR Camera is a versatile camera that allows you to capture stunning photos and videos. It features a 24.2MP sensor, 1080p video recording, a 3-inch LCD screen, and supports interchangeable lenses. The FotoSnap DSLR Camera is priced at $599.99 and also comes with a 1-year warranty.\n",
    "\n",
    "As for TVs and TV-related products, we have a range of options available. Some of our popular TV models include the CineView 4K TV, CineView 8K TV, and CineView OLED TV. We also have home theater systems like the SoundMax Home Theater and SoundMax Soundbar. Could you please let me know your specific requirements or preferences so that I can assist you better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889c7b4a",
   "metadata": {},
   "source": [
    "#### How can you evaluate if this is a good answer or not? \n",
    "Seems like there are lots of possible good answers. \n",
    "One way to evaluate this is to write a rubric, \n",
    "meaning a set of guidelines, to evaluate this \n",
    "answer on different dimensions, and then use that to \n",
    "decide whether or not you're satisfied with this answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd397f10",
   "metadata": {},
   "source": [
    "### Evaluate the LLM's answer to the user with a rubric, based on the extracted product information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57df5b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_prod_info = {\n",
    "    'customer_msg': customer_msg,\n",
    "    'context': product_info\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41032e5d",
   "metadata": {},
   "source": [
    " So, let me create a little data structure to \n",
    "store the customer message as well as the product info. \n",
    "So here, I'm going to specify a prompt for evaluating the \n",
    "assistant answer using what's called a rubric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b63bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_with_rubric(test_set, assistant_answer):\n",
    "\n",
    "    cust_msg = test_set['customer_msg']\n",
    "    context = test_set['context']\n",
    "    completion = assistant_answer\n",
    "    \n",
    "    system_message = \"\"\"\\\n",
    "    You are an assistant that evaluates how well the customer service agent \\\n",
    "    answers a user question by looking at the context that the customer service \\\n",
    "    agent is using to generate its response. \n",
    "    \"\"\"\n",
    "\n",
    "    user_message = f\"\"\"\\\n",
    "You are evaluating a submitted answer to a question based on the context \\\n",
    "that the agent uses to answer the question.\n",
    "Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]: {cust_msg}\n",
    "    ************\n",
    "    [Context]: {context}\n",
    "    ************\n",
    "    [Submission]: {completion}\n",
    "    ************\n",
    "    [END DATA]\n",
    "\n",
    "Compare the factual content of the submitted answer with the context. \\\n",
    "Ignore any differences in style, grammar, or punctuation.\n",
    "Answer the following questions:\n",
    "    - Is the Assistant response based only on the context provided? (Y or N)\n",
    "    - Does the answer include information that is not provided in the context? (Y or N)\n",
    "    - Is there any disagreement between the response and the context? (Y or N)\n",
    "    - Count how many questions the user asked. (output a number)\n",
    "    - For each question that the user asked, is there a corresponding answer to it?\n",
    "      Question 1: (Y or N)\n",
    "      Question 2: (Y or N)\n",
    "      ...\n",
    "      Question N: (Y or N)\n",
    "    - Of the number of questions asked, how many of these questions were addressed by the answer? (output a number)\n",
    "\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "\n",
    "    response = get_completion_from_messages(messages)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98f5abe",
   "metadata": {},
   "source": [
    "So, this is called a rubric, and this specifies what \n",
    "we think the answer should get right for \n",
    "us to consider it a good answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9777cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_output = eval_with_rubric(cust_prod_info, assistant_answer)\n",
    "print(evaluation_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6620fac",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "- Is the Assistant response based only on the context provided? (Y or N)\n",
    "\n",
    "Y\n",
    "\n",
    "- Does the answer include information that is not provided in the context? (Y or N)\n",
    "\n",
    "N\n",
    "\n",
    "- Is there any disagreement between the response and the context? (Y or N)\n",
    "\n",
    "N\n",
    "\n",
    "- Count how many questions the user asked. (output a number)\n",
    "\n",
    "2\n",
    "\n",
    "- For each question that the user asked, is there a corresponding answer to it?\n",
    "\n",
    "Question 1: Y\n",
    "\n",
    "Question 2: Y\n",
    "\n",
    "- Of the number of questions asked, how many of these questions were addressed by the answer? (output a number)\n",
    "\n",
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd0209",
   "metadata": {},
   "source": [
    "So we would look at this output and maybe conclude that this \n",
    "is a pretty good response. \n",
    "And one note, here I'm using the ChatGPT 3.5 Turbo \n",
    "model for this evaluation. \n",
    "For a more robust evaluation, it might be worth considering using \n",
    "GPT-4 because even if you deploy 3.5 Turbo in production and generate a \n",
    "lot of text, if your evaluation is a more \n",
    "sporadic exercise, then it may be prudent to pay for the somewhat \n",
    "more expensive GPT-4 API call to get a more rigorous evaluation \n",
    "of the output. \n",
    "One design pattern that I hope you can take away \n",
    "from this is that when you can specify a rubric, \n",
    "meaning a list of criteria by which to \n",
    "evaluate an LLM output, then you can actually \n",
    "use another API call to evaluate your first LLM output. \n",
    "There's one other design pattern that could be useful \n",
    "for some applications, which is if you can \n",
    "specify an ideal response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45190a7a",
   "metadata": {},
   "source": [
    "The \"BLEU\" (Bilingual Evaluation Understudy) score, which is a metric used to evaluate the quality of machine-generated text, such as machine translation or language generation models, like Large Language Models (LLMs).\n",
    "\n",
    "BLEU measures the similarity between the generated text and a reference (human-written) text. It compares n-grams (contiguous sequences of n items, like words or characters) in the generated text to the n-grams in the reference text. The basic idea is that if the generated text has n-grams that match those in the reference text, it's likely to be a good translation or a high-quality generated text.\n",
    "\n",
    "The BLEU score ranges from 0 to 1, where higher values indicate better quality. It's important to note that BLEU is a relatively simple metric and has some limitations. For example, it doesn't capture the overall fluency, coherence, or semantic correctness of the generated text. Also, it heavily relies on exact n-gram matches, which might not always be the best representation of quality, especially for longer sentences or when dealing with paraphrased content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a40ef",
   "metadata": {},
   "source": [
    "### Evaluate the LLM's answer to the user based on an \"ideal\" / \"expert\" (human generated) answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e360ab0f",
   "metadata": {},
   "source": [
    "So this is if you have an expert human \n",
    "customer service representative write a really good answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_ideal = {\n",
    "    'customer_msg': \"\"\"\\\n",
    "tell me about the smartx pro phone and the fotosnap camera, the dslr one.\n",
    "Also, what TVs or TV related products do you have?\"\"\",\n",
    "    'ideal_answer':\"\"\"\\\n",
    "Of course!  The SmartX ProPhone is a powerful \\\n",
    "smartphone with advanced camera features. \\\n",
    "For instance, it has a 12MP dual camera. \\\n",
    "Other features include 5G wireless and 128GB storage. \\\n",
    "It also has a 6.1-inch display.  The price is $899.99.\n",
    "\n",
    "The FotoSnap DSLR Camera is great for \\\n",
    "capturing stunning photos and videos. \\\n",
    "Some features include 1080p video, \\\n",
    "3-inch LCD, a 24.2MP sensor, \\\n",
    "and interchangeable lenses. \\\n",
    "The price is 599.99.\n",
    "\n",
    "For TVs and TV related products, we offer 3 TVs \\\n",
    "\n",
    "\n",
    "All TVs offer HDR and Smart TV.\n",
    "\n",
    "The CineView 4K TV has vibrant colors and smart features. \\\n",
    "Some of these features include a 55-inch display, \\\n",
    "'4K resolution. It's priced at 599.\n",
    "\n",
    "The CineView 8K TV is a stunning 8K TV. \\\n",
    "Some features include a 65-inch display and \\\n",
    "8K resolution.  It's priced at 2999.99\n",
    "\n",
    "The CineView OLED TV lets you experience vibrant colors. \\\n",
    "Some features include a 55-inch display and 4K resolution. \\\n",
    "It's priced at 1499.99.\n",
    "\n",
    "We also offer 2 home theater products, both which include bluetooth.\\\n",
    "The SoundMax Home Theater is a powerful home theater system for \\\n",
    "an immmersive audio experience.\n",
    "Its features include 5.1 channel, 1000W output, and wireless subwoofer.\n",
    "It's priced at 399.99.\n",
    "\n",
    "The SoundMax Soundbar is a sleek and powerful soundbar.\n",
    "It's features include 2.1 channel, 300W output, and wireless subwoofer.\n",
    "It's priced at 199.99\n",
    "\n",
    "Are there any questions additional you may have about these products \\\n",
    "that you mentioned here?\n",
    "Or may do you have other questions I can help you with?\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b27e7f",
   "metadata": {},
   "source": [
    "### Check if the LLM's response agrees with or disagrees with the expert answer\n",
    "\n",
    "This evaluation prompt is from the [OpenAI evals](https://github.com/openai/evals/blob/main/evals/registry/modelgraded/fact.yaml) project.\n",
    "\n",
    "[BLEU score](https://en.wikipedia.org/wiki/BLEU): another way to evaluate whether two pieces of text are similar or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d11537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_vs_ideal(test_set, assistant_answer):\n",
    "\n",
    "    cust_msg = test_set['customer_msg']\n",
    "    ideal = test_set['ideal_answer']\n",
    "    completion = assistant_answer\n",
    "    \n",
    "    system_message = \"\"\"\\\n",
    "    You are an assistant that evaluates how well the customer service agent \\\n",
    "    answers a user question by comparing the response to the ideal (expert) response\n",
    "    Output a single letter and nothing else. \n",
    "    \"\"\"\n",
    "\n",
    "    user_message = f\"\"\"\\\n",
    "You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]: {cust_msg}\n",
    "    ************\n",
    "    [Expert]: {ideal}\n",
    "    ************\n",
    "    [Submission]: {completion}\n",
    "    ************\n",
    "    [END DATA]\n",
    "\n",
    "Compare the factual content of the submitted answer with the expert answer. \\\n",
    "Ignore any differences in style, grammar, or punctuation.\n",
    "    The submitted answer may either be a subset or superset of the expert answer, \\\n",
    "    or it may conflict with it. Determine which case applies. \\\n",
    "    Answer the question by selecting one of the following options:\n",
    "    (A) The submitted answer is a subset of the expert answer and is fully consistent with it.\n",
    "    (B) The submitted answer is a superset of the expert answer and is fully consistent with it.\n",
    "    (C) The submitted answer contains all the same details as the expert answer.\n",
    "    (D) There is a disagreement between the submitted answer and the expert answer.\n",
    "    (E) The answers differ, but these differences don't matter from the perspective of factuality.\n",
    "  choice_strings: ABCDE\n",
    "\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "\n",
    "    response = get_completion_from_messages(messages)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e1b320",
   "metadata": {},
   "source": [
    "Here's the prompt we can use, which is. We're going to use an LLM \n",
    "and tell it to be an assistant that evaluates how well the \n",
    "customer service agent answers a user question by \n",
    "comparing the response, that was the automatically generated \n",
    "one, to the ideal (expert) human written \n",
    "response. \n",
    "So we're going to give it the data, which is what was the customer request, what \n",
    "is the expert written ideal response, and then what did our \n",
    "LLM actually output. \n",
    "And this rubric comes from the OpenAI open source evals framework, \n",
    "which is a fantastic framework with many evaluation methods \n",
    "contributed both by OpenAI developers and \n",
    "by the broader open source community. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3998d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assistant_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f976d5b3",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "Sure! Let me provide you with some information about the SmartX ProPhone and the FotoSnap DSLR Camera.\n",
    "\n",
    "The SmartX ProPhone is a powerful smartphone with advanced camera features. It has a 6.1-inch display, 128GB storage, a 12MP dual camera, and supports 5G connectivity. The SmartX ProPhone is priced at $899.99 and comes with a 1-year warranty.\n",
    "\n",
    "The FotoSnap DSLR Camera is a versatile camera that allows you to capture stunning photos and videos. It features a 24.2MP sensor, 1080p video recording, a 3-inch LCD screen, and supports interchangeable lenses. The FotoSnap DSLR Camera is priced at $599.99 and also comes with a 1-year warranty.\n",
    "\n",
    "As for TVs and TV-related products, we have a range of options available. Some of our popular TV models include the CineView 4K TV, CineView 8K TV, and CineView OLED TV. We also have home theater systems like the SoundMax Home Theater and SoundMax Soundbar. Could you please let me know your specific requirements or preferences so that I can assist you better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b6add",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_vs_ideal(test_set_ideal, assistant_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84846924",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "'A'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d96b787",
   "metadata": {},
   "source": [
    "This assistant answer is much shorter than the \n",
    "long expert answer up top, but it does hopefully is consistent. \n",
    "Once again, I'm using GPT-3.5 Turbo in this example, but to get \n",
    "a more rigorous evaluation, it might make sense to use GPT-4 in your own \n",
    "application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a01c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_answer_2 = \"life is like a box of chocolates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b46c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_vs_ideal(test_set_ideal, assistant_answer_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed8080",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "'D'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ef2eb",
   "metadata": {},
   "source": [
    "In summary, there are two key points for evaluating LLM systems:\n",
    "\n",
    "1. Without an expert-provided ideal answer, you can use a rubric to evaluate one LLM's output against another LLM's output.\n",
    "\n",
    "2. With an expert-provided ideal answer, you can compare the LLM's output to the expert's answer, which helps in assessing the similarity and quality of the LLM's responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7b7895",
   "metadata": {},
   "source": [
    "\n",
    "These evaluation methods are valuable during development and while the system is running, allowing continuous monitoring and improvement of the LLM's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
