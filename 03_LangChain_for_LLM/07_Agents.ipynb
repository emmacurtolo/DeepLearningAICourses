{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf222bb",
   "metadata": {},
   "source": [
    "# LangChain: Agents\n",
    "\n",
    "## Outline:\n",
    "\n",
    "* Using built in LangChain tools: DuckDuckGo search and Wikipedia\n",
    "* Defining your own tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d1022b",
   "metadata": {},
   "source": [
    "Sometimes people think of a large language model \n",
    "as a knowledge store, as if it's learned to memorize a \n",
    "lot of information, maybe off the internet, so \n",
    "when you ask it a question, it can answer the question. \n",
    "But I think a even more useful way to think of a \n",
    "large language model is sometimes as a reasoning engine, \n",
    "in which you can give it chunks of text or other sources \n",
    "of information. \n",
    "And then the large language model, LLM, will maybe use \n",
    "this background knowledge that's learned off the internet, but \n",
    "to use the new information you give it \n",
    "to help you answer questions or reason through \n",
    "content or decide even what to do next. And that's what \n",
    "LangChain's Agents framework helps you to do. \n",
    " \n",
    "Agents are probably my favorite part of LangChain. \n",
    "I think they're also one of the most powerful parts, but \n",
    "they're also one of the newer parts. So we're \n",
    "seeing a lot of stuff emerge here that's really new to \n",
    "everyone in the field. And so this should be a very exciting lesson \n",
    "as we dive into what agents are, how to create and \n",
    "how to use agents, how to equip them with different types of \n",
    "tools, like search engines that come built into LangChain, \n",
    "and then also how to create your own \n",
    "tools, so that you can let agents interact with any \n",
    "data stores, any APIs, any functions that you \n",
    "might want them to. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec89981",
   "metadata": {},
   "source": [
    "## Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3d816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e477bb",
   "metadata": {},
   "source": [
    "## Built-in LangChain tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f71b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6843be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing packages\n",
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.python import PythonREPL\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b414145f",
   "metadata": {},
   "source": [
    "We're going to initialize a language model. We're going to use Chat OpenAI, and importantly, we're going to set the temperature equal to zero. This is important because we're going to be using the language model as the reasoning.\n",
    "\n",
    "Engine of an agent where it's connecting to other sources of data and computation. And so we want this reasoning engine to be as good and as precise as possible. And so we're going to set it to zero to get rid of any randomness that might arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f397439",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d93020",
   "metadata": {},
   "source": [
    "The two tools that we're going to load are the LLM math tool and the Wikipedia tool. The LLM math tool is actually a chain.\n",
    "\n",
    "Itself, which uses a language model in conjunction with a calculator to do math problems. The Wikipedia tool is an API that connects to Wikipedia, allowing you to run search queries against Wikipedia and get back results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b3078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"llm-math\",\"wikipedia\"], llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3a894c",
   "metadata": {},
   "source": [
    "We're going to initialize the agent with the tools, the language model and then an agent type. Here we're going to use chat zero shot react description. The important things to note here are first chat this is an agent that has been optimized to work with chat models and second react.\n",
    "\n",
    "This is a prompting technique designed to get the best reasoning performance out of language models. We're also going to pass in handle parsing errors equals true. This is useful when the language model might output something that is not able to be parsed into an action and action input which is the desired output.\n",
    "\n",
    "When this happens we'll actually pass the misformatted text back to the language model and ask it to correct itself. And finally we're going to pass in verbose equals true. This is going to print out a bunch of steps that makes it really clear to us in the jupyter notebook what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an agent with tools, llm and an agent type\n",
    "agent= initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b25a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\"What is the 25% of 300?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95e9a66",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "![Agent](immagini/29_agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e80d083",
   "metadata": {},
   "source": [
    "So we can see here that when it enters the agent executor chain that it first thinks about what it needs to do.\n",
    "\n",
    "So it has a thought, it then has an action. And this action is actually a JSON Blob, corresponding to two things an action and an action input. The action corresponds to the tool to use.\n",
    "\n",
    "So here it says Calculator. The action input is the input to that tool and here it's a string of 300 times 00.25. Next we can see that there's observation with answer in a separate color.\n",
    "\n",
    "This observation answer equals 75.0 is actually coming from the calculator tool itself. Next we go back to the language model.\n",
    "\n",
    "When the text turns to green, we have the answer to the question final answer 75.0 and that's the output that we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fb29ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Tom M. Mitchell is an American computer scientist \\\n",
    "and the Founders University Professor at Carnegie Mellon University (CMU)\\\n",
    "what book did he write?\"\n",
    "result = agent(question) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a6866",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "![Agent](immagini/30_agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7aea98",
   "metadata": {},
   "source": [
    "The observation that comes back from Wikipedia is actually two results, two pages.\n",
    "\n",
    "As there's two different Tom Mitchells. We can see the first one covers the computer scientist and the second one it looks like it's an Australian footballer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f175ef",
   "metadata": {},
   "source": [
    "## Python Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e451aed5",
   "metadata": {},
   "source": [
    "If you've seen things like Copilot or even Chat GPT with the Code interpreter plugin enabled, one of the things they're doing is they're using the language model to write code and then executing that code.\n",
    "\n",
    "We're going to create a Python agent and we're going to use the same LLM as before, and we're going to give it a tool, the Python REPL Tool. A REPL is basically a way to interact with code.\n",
    "\n",
    "You can think of it as a jupyter notebook. So the agent can execute code with this REPL. It will then run, and then we'll get back some results, and those results will be passed back into the agent so it can decide what to do next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b5b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_python_agent(\n",
    "    llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3aaa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problem that we're going to have this agent solve is \n",
    "# we're going to give it a list of names and then ask it to sort them.\n",
    "\n",
    "customer_list = [[\"Harrison\", \"Chase\"], \n",
    "                 [\"Lang\", \"Chain\"],\n",
    "                 [\"Dolly\", \"Too\"],\n",
    "                 [\"Elle\", \"Elem\"], \n",
    "                 [\"Geoff\",\"Fusion\"], \n",
    "                 [\"Trance\",\"Former\"],\n",
    "                 [\"Jen\",\"Ayai\"]\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda86998",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(f\"\"\"Sort these customers by \\\n",
    "last name and then first name \\\n",
    "and print the output: {customer_list}\"\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec08c24",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "![Agent](immagini/31_agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a93b67",
   "metadata": {},
   "source": [
    "We're asking it to print the output so that it can actually see what the result is. These printed statements are what's going to be fed back into the language model later on, so it can reason about the output of the code that it just ran."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edf8b05",
   "metadata": {},
   "source": [
    "#### View detailed outputs of the chains\n",
    "\n",
    "Let's dig a little bit deeper and run this with Langchain debug set to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f144fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug=True\n",
    "agent.run(f\"\"\"Sort these customers by \\\n",
    "last name and then first name \\\n",
    "and print the output: {customer_list}\"\"\") \n",
    "langchain.debug=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da0afd7",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "```python\n",
    "[chain/start] [1:chain:AgentExecutor] Entering Chain run with input:\n",
    "{\n",
    "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\"\n",
    "}\n",
    "[chain/start] [1:chain:AgentExecutor > 2:chain:LLMChain] Entering Chain run with input:\n",
    "{\n",
    "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n",
    "  \"agent_scratchpad\": \"\",\n",
    "  \"stop\": [\n",
    "    \"\\nObservation:\",\n",
    "    \"\\n\\tObservation:\"\n",
    "  ]\n",
    "}\n",
    "[llm/start] [1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
    "{\n",
    "  \"prompts\": [\n",
    "    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython REPL: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nThought:\"\n",
    "  ]\n",
    "}\n",
    "[llm/end] [1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] [3.82s] Exiting LLM run with output:\n",
    "{\n",
    "  \"generations\": [\n",
    "    [\n",
    "      {\n",
    "        \"text\": \"I can use the `sorted()` function to sort the list of customers. I will need to provide a key function that specifies the sorting criteria.\\nAction: Python REPL\\nAction Input: `customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]`\",\n",
    "        \"generation_info\": null,\n",
    "        \"message\": {\n",
    "          \"content\": \"I can use the `sorted()` function to sort the list of customers. I will need to provide a key function that specifies the sorting criteria.\\nAction: Python REPL\\nAction Input: `customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]`\",\n",
    "          \"additional_kwargs\": {},\n",
    "          \"example\": false\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  ],\n",
    "  \"llm_output\": {\n",
    "    \"token_usage\": {\n",
    "      \"prompt_tokens\": 326,\n",
    "      \"completion_tokens\": 92,\n",
    "      \"total_tokens\": 418\n",
    "    },\n",
    "    \"model_name\": \"gpt-3.5-turbo\"\n",
    "  }\n",
    "}\n",
    "[chain/end] [1:chain:AgentExecutor > 2:chain:LLMChain] [3.82s] Exiting Chain run with output:\n",
    "{\n",
    "  \"text\": \"I can use the `sorted()` function to sort the list of customers. I will need to provide a key function that specifies the sorting criteria.\\nAction: Python REPL\\nAction Input: `customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]`\"\n",
    "}\n",
    "[tool/start] [1:chain:AgentExecutor > 4:tool:Python REPL] Entering Tool run with input:\n",
    "\"`customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]`\"\n",
    "[tool/end] [1:chain:AgentExecutor > 4:tool:Python REPL] [0.232ms] Exiting Tool run with output:\n",
    "\"\"\n",
    "[chain/start] [1:chain:AgentExecutor > 5:chain:LLMChain] Entering Chain run with input:\n",
    "{\n",
    "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n",
    "  \"agent_scratchpad\": \"I can use the `sorted()` function to sort the list of customers. I will need to provide a key function that specifies the sorting criteria.\\nAction: Python REPL\\nAction Input: `customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]`\\nObservation: \\nThought:\",\n",
    "  \"stop\": [\n",
    "    \"\\nObservation:\",\n",
    "    \"\\n\\tObservation:\"\n",
    "  ]\n",
    "}\n",
    "[llm/start] [1:chain:AgentExecutor > 5:chain:LLMChain > 6:llm:ChatOpenAI] Entering LLM run with input:\n",
    "{\n",
    "  \"prompts\": [\n",
    "    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython REPL: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nThought:I can use the `sorted()` function to sort the list of customers. I will need to provide a key function that specifies the sorting criteria.\\nAction: Python REPL\\nAction Input: `customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]`\\nObservation: \\nThought:\"\n",
    "  ]\n",
    "}\n",
    "[llm/end] [1:chain:AgentExecutor > 5:chain:LLMChain > 6:llm:ChatOpenAI] [2.22s] Exiting LLM run with output:\n",
    "{\n",
    "  \"generations\": [\n",
    "    [\n",
    "      {\n",
    "        \"text\": \"I will use the `sorted()` function to sort the list of customers. I will provide a lambda function as the key argument to specify the sorting criteria.\\nAction: Python REPL\\nAction Input: `sorted(customers, key=lambda x: (x[1], x[0]))`\",\n",
    "        \"generation_info\": null,\n",
    "        \"message\": {\n",
    "          \"content\": \"I will use the `sorted()` function to sort the list of customers. I will provide a lambda function as the key argument to specify the sorting criteria.\\nAction: Python REPL\\nAction Input: `sorted(customers, key=lambda x: (x[1], x[0]))`\",\n",
    "          \"additional_kwargs\": {},\n",
    "          \"example\": false\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  ],\n",
    "  \"llm_output\": {\n",
    "    \"token_usage\": {\n",
    "      \"prompt_tokens\": 423,\n",
    "      \"completion_tokens\": 58,\n",
    "      \"total_tokens\": 481\n",
    "    },\n",
    "    \"model_name\": \"gpt-3.5-turbo\"\n",
    "  }\n",
    "}\n",
    "[chain/end] [1:chain:AgentExecutor > 5:chain:LLMChain] [2.22s] Exiting Chain run with output:\n",
    "{\n",
    "  \"text\": \"I will use the `sorted()` function to sort the list of customers. I will provide a lambda function as the key argument to specify the sorting criteria.\\nAction: Python REPL\\nAction Input: `sorted(customers, key=lambda x: (x[1], x[0]))`\"\n",
    "}\n",
    "[tool/start] [1:chain:AgentExecutor > 7:tool:Python REPL] Entering Tool run with input:\n",
    "\"`sorted(customers, key=lambda x: (x[1], x[0]))`\"\n",
    "[tool/end] [1:chain:AgentExecutor > 7:tool:Python REPL] [0.219ms] Exiting Tool run with output:\n",
    "\"\"\n",
    "[chain/start] [1:chain:AgentExecutor > 8:chain:LLMChain] Entering Chain run with input:\n",
    "{\n",
    "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n",
    "  \"agent_scratchpad\": \"I can use the `sorted()` function to sort the list of customers. I will need to provide a key function that specifies the sorting criteria.\\nAction: Python REPL\\nAction Input: `customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]`\\nObservation: \\nThought:I will use the `sorted()` function to sort the list of customers. I will provide a lambda function as the key argument to specify the sorting criteria.\\nAction: Python REPL\\nAction Input: `sorted(customers, key=lambda x: (x[1], x[0]))`\\nObservation: \\nThought:\",\n",
    "  \"stop\": [\n",
    "    \"\\nObservation:\",\n",
    "    \"\\n\\tObservation:\"\n",
    "  ]\n",
    "}\n",
    "[llm/start] [1:chain:AgentExecutor > 8:chain:LLMChain > 9:llm:ChatOpenAI] Entering LLM run with input:\n",
    "{\n",
    "  \"prompts\": [\n",
    "    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython REPL: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nThought:I can use the `sorted()` function to sort the list of customers. I will need to provide a key function that specifies the sorting criteria.\\nAction: Python REPL\\nAction Input: `customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]`\\nObservation: \\nThought:I will use the `sorted()` function to sort the list of customers. I will provide a lambda function as the key argument to specify the sorting criteria.\\nAction: Python REPL\\nAction Input: `sorted(customers, key=lambda x: (x[1], x[0]))`\\nObservation: \\nThought:\"\n",
    "  ]\n",
    "}\n",
    "[llm/end] [1:chain:AgentExecutor > 8:chain:LLMChain > 9:llm:ChatOpenAI] [2.72s] Exiting LLM run with output:\n",
    "{\n",
    "  \"generations\": [\n",
    "    [\n",
    "      {\n",
    "        \"text\": \"The customers have been sorted by last name and then first name.\\nFinal Answer: [['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\",\n",
    "        \"generation_info\": null,\n",
    "        \"message\": {\n",
    "          \"content\": \"The customers have been sorted by last name and then first name.\\nFinal Answer: [['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\",\n",
    "          \"additional_kwargs\": {},\n",
    "          \"example\": false\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  ],\n",
    "  \"llm_output\": {\n",
    "    \"token_usage\": {\n",
    "      \"prompt_tokens\": 486,\n",
    "      \"completion_tokens\": 67,\n",
    "      \"total_tokens\": 553\n",
    "    },\n",
    "    \"model_name\": \"gpt-3.5-turbo\"\n",
    "  }\n",
    "}\n",
    "[chain/end] [1:chain:AgentExecutor > 8:chain:LLMChain] [2.72s] Exiting Chain run with output:\n",
    "{\n",
    "  \"text\": \"The customers have been sorted by last name and then first name.\\nFinal Answer: [['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\"\n",
    "}\n",
    "[chain/end] [1:chain:AgentExecutor] [8.76s] Exiting Chain run with output:\n",
    "{\n",
    "  \"output\": \"[['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f349803a",
   "metadata": {},
   "source": [
    "This debug mode can also be used to highlight what's going wrong, as shown above in the Wikipedia example. Sometimes agents act a little funny and so having all this information is really helpful for understanding what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a046fda8",
   "metadata": {},
   "source": [
    "## Define your own tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2e065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aa9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716e2d68",
   "metadata": {},
   "source": [
    "A big power of agents is that you can connect it to your own sources of information, your own APIs, your own data. So here we're going to go over how you can create a custom tool so that you can connect it to whatever you want.\n",
    "\n",
    "Let's make a tool that's going to tell us what the current data is. First, we're going to import this tool decorator. This can be applied to any function and it turns it into a tool that Langchang can use.\n",
    "\n",
    "Next, we're going to write a function called Time which takes in any text string. We're not really going to use that and it's going to return today's date by calling date Time. In addition to the name of the function, we're also going to write a really detailed doc string.\n",
    "\n",
    "That's because this is what the agent will use to know when it should call this tool and how it should call this tool. For example, here we say that the input should always be an empty string. That's because we don't use it if we have more stringent requirements on what the input should be.\n",
    "\n",
    "For example, if we have a function that should always take in a search query or a SQL statement, you'll want to make sure to mention that here we're now going to create another agent. This time we're adding the Time tool to the list of existing tools. And finally, let's call the agent and ask it what the date today is.\n",
    "\n",
    "It recognizes that it needs to use the Time tool which it specifies. Here it has the action input as an empty string. This is great.\n",
    "\n",
    "This is what we told it to do. And then it returns with an observation. And then finally the language model takes that observation and responds to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def time(text: str) -> str:\n",
    "    \"\"\"Returns todays date, use this for any \\\n",
    "    questions related to knowing todays date. \\\n",
    "    The input should always be an empty string, \\\n",
    "    and this function will always return todays \\\n",
    "    date - any date mathmatics should occur \\\n",
    "    outside this function.\"\"\"\n",
    "    return str(date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c50442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent= initialize_agent(\n",
    "    tools + [time], \n",
    "    llm, \n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67899e",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "![Agent](immagini/33_agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8690b412",
   "metadata": {},
   "source": [
    "**Note**: \n",
    "\n",
    "The agent will sometimes come to the wrong conclusion (agents are a work in progress!). \n",
    "\n",
    "If it does, please try running it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e556db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = agent(\"whats the date today?\") \n",
    "except: \n",
    "    print(\"exception on external access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e499cc4",
   "metadata": {},
   "source": [
    "```python\n",
    "> Entering new AgentExecutor chain...\n",
    "Question: What's the date today?\n",
    "Thought: I can use the `time` tool to get the current date.\n",
    "Action:\n",
    "```\n",
    "{\n",
    "  \"action\": \"time\",\n",
    "  \"action_input\": \"\"\n",
    "}\n",
    "```\n",
    "Observation: 2023-08-02\n",
    "Thought:\n",
    "Observation: Invalid or incomplete response\n",
    "Thought:I made a mistake in my previous response. I apologize for the confusion. Let me correct it and provide the final answer.\n",
    "\n",
    "Question: What's the date today?\n",
    "Thought: I can use the `time` tool to get the current date.\n",
    "Action:\n",
    "```\n",
    "{\n",
    "  \"action\": \"time\",\n",
    "  \"action_input\": \"\"\n",
    "}\n",
    "```\n",
    "\n",
    "Observation: 2023-08-02\n",
    "Thought:I now know the final answer.\n",
    "Final Answer: The date today is August 2, 2023.\n",
    "\n",
    "> Finished chain.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
