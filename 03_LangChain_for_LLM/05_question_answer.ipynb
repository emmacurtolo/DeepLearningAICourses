{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66cae983",
   "metadata": {},
   "source": [
    "# LangChain: Q&A over Documents\n",
    "\n",
    "An example might be a tool that would allow you to query a product catalog for items of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbb8e82",
   "metadata": {},
   "source": [
    "One of the most common, complex applications that \n",
    "people are building using an LLM is a system that can answer \n",
    "questions on top of or about a document. \n",
    "So, given a piece of text, maybe extracted from a \n",
    "PDF file or from a webpage or from some company's \n",
    "intranet internal document collection, can you use an LLM to answer \n",
    "questions about the content of those documents to help \n",
    "users gain a deeper understanding and \n",
    "get access to the information that they need? \n",
    "This is really powerful because it starts to combine \n",
    "these language models with data that they weren't \n",
    "originally trained on. So it makes them much \n",
    "more flexible and adaptable to your use case. It's also \n",
    "really exciting because we'll start to move beyond language models, prompts, and output \n",
    "parsers and start introducing some more of the key components \n",
    "of LangChain, such as embedding models and vector stores. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36202e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12f950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14a3f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "# This will do retrieval over some documents.\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.document_loaders import CSVLoader\n",
    "# used to load some proprietary data that we're going to combine with the language model.\n",
    "# In this case it's going to be in a CSV.\n",
    "\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "# We're going to import a vector store.\n",
    "# This is really nice because it's an in-memory vector store and it doesn't require connecting to an \n",
    "# external database of any kind so it makes it really easy to get started. \n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "# common utilities for displaying information in Jupyter notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e2d8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file)\n",
    "# Here we're going to initialize a loader, the CSV loader, with a path to this file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb466d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "# We're next going to import an index, the \"VectorStoreIndexCreator\". \n",
    "# This will help us create a vector store really easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ee78e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install docarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b69d2d",
   "metadata": {},
   "source": [
    "To create it, we're going to specify two things. \n",
    "First, we're going to specify the vector store class. \n",
    "As mentioned before, we're going to use this vector store, as \n",
    "it's a particularly easy one to get started with. \n",
    "After it's been created, we're then going to call \"from_loaders\", which takes in \n",
    "a list of document loaders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e91bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4981412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"Please list all your shirts with sun protection \\\n",
    "in a table in markdown and summarize each one.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3899a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index.query(query)\n",
    "# create a response using \"index.query\" and pass in this query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a660b10",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Name</th>\n",
    "      <th>Description</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Men's Tropical Plaid Short-Sleeve Shirt</td>\n",
    "      <td>UPF 50+ rated, 100% polyester, wrinkle-resistant, front and back cape venting, two front bellows pockets</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Men's Plaid Tropic Shirt, Short-Sleeve</td>\n",
    "      <td>UPF 50+ rated, 52% polyester and 48% nylon, machine washable and dryable, front and back cape venting, two front bellows pockets</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Men's TropicVibe Shirt, Short-Sleeve</td>\n",
    "      <td>UPF 50+ rated, 71% nylon, 29% polyester, 100% polyester knit mesh, wrinkle-resistant, front and back cape venting, two front bellows pockets</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Sun Shield Shirt by</td>\n",
    "      <td>UPF 50+ rated, 78% nylon, 22% Lycra Xtra Life fiber, wicks moisture, fits comfortably over swimsuit, abrasion resistant</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "```\n",
    "All of the shirts provide UPF 50+ sun protection, blocking 98% of the sun's harmful rays. They are all wrinkle-resistant and have front and back cape venting and two front bellows pockets. The Men's Tropical Plaid Short-\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa55d71",
   "metadata": {},
   "source": [
    "We've gotten back a table in markdown with names \n",
    "and descriptions for all shirts with sun protection. \n",
    "We've also got a nice little summary that the \n",
    "language model has provided us. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df06f9d6",
   "metadata": {},
   "source": [
    "1. **LLM (Large Language Model):** Refers to a large-scale language model, like GPT-3, capable of processing and generating human-like text.\n",
    "\n",
    "2. **Question Answering System:** An application built using LLMs that can answer questions based on a given document's content, helping users gain a deeper understanding of the information.\n",
    "\n",
    "3. **Embeddings:** Numerical representations of pieces of text that capture their semantic meaning, allowing for comparison of similarity between pieces of text.\n",
    "\n",
    "4. **Vector Store/Database:** A storage system for storing embeddings of various documents, enabling efficient retrieval and comparison of relevant texts.\n",
    "\n",
    "5. **Retriever:** A component used to fetch documents based on an incoming query.\n",
    "\n",
    "6. **ChatOpenAI:** A language model used for text generation and natural language responses.\n",
    "\n",
    "7. **LangChain:** A framework for composing and executing complex language model workflows.\n",
    "\n",
    "8. **Stuff Method:** A simple approach in LangChain where all documents are combined into one prompt and sent to the language model for generating a single response.\n",
    "\n",
    "9. **Map_reduce Method:** A method in LangChain where documents are processed independently in parallel and then summarized into a final answer.\n",
    "\n",
    "10. **Refine Method:** A method in LangChain where answers are built iteratively based on previous responses, allowing for combining information and generating longer answers.\n",
    "\n",
    "11. **Map_rerank Method:** An experimental method in LangChain where individual calls to the language model are made for each document, and the highest scoring response is selected.\n",
    "\n",
    "These concepts are relevant to building a powerful and flexible system for question-answering tasks using LLMs and vector stores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454aaec8",
   "metadata": {},
   "source": [
    "![Question](immagini/15_question.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e3d3cb",
   "metadata": {},
   "source": [
    "We want to use language models and \n",
    "combine it with a lot of our documents. \n",
    "But there's a key issue. \n",
    "Language models can only inspect a few thousand \n",
    "words at a time. \n",
    "So if we have really large documents, how can we get \n",
    "the language model to answer questions about everything \n",
    "that's in there? \n",
    "This is where embeddings and vector stores come into play. First, \n",
    "let's talk about embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cc214a",
   "metadata": {},
   "source": [
    "Embeddings create numerical representations \n",
    "for pieces of text. \n",
    "This numerical representation captures the semantic \n",
    "meaning of the piece of text that it's been run over. \n",
    "Pieces of text with similar content will have similar vectors. \n",
    "This lets us compare pieces of text in the vector space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a014fa5c",
   "metadata": {},
   "source": [
    "![Question](immagini/16_question.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6944f7e",
   "metadata": {},
   "source": [
    "In the example, we can see that \n",
    "we have three sentences. \n",
    "The first two are about pets, while the third is about a car. \n",
    "If we look at the representation in the numeric space, \n",
    "we can see that when we compare the two vectors on the \n",
    "pieces of text corresponding to the sentences about pets, they're \n",
    "very similar. \n",
    "While if we compare it to the one that talks about a car, \n",
    "they're not similar at all. \n",
    "This will let us easily figure out which pieces of \n",
    "text are like each other, which will be very useful as \n",
    "we think about which pieces of text we want to include when \n",
    "passing to the language model to answer a question. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59a5854",
   "metadata": {},
   "source": [
    "![Question](immagini/17_question.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b508888",
   "metadata": {},
   "source": [
    "The next component that we're going to cover is \n",
    "the vector database. \n",
    "A vector database is a way to store these \n",
    "vector representations that we created in the previous step. \n",
    "The way that we create this vector database \n",
    "is we populate it with chunks of text \n",
    "coming from incoming documents. \n",
    "When we get a big incoming document, we're first going to break it \n",
    "up into smaller chunks. \n",
    "This helps create pieces of text that are \n",
    "smaller than the original document, which is useful because \n",
    "we may not be able to pass the whole document to the \n",
    "language model. So we want to create these small chunks \n",
    "so we can only pass the most relevant \n",
    "ones to the language model. \n",
    "We then create an embedding for each of these chunks, \n",
    "and then we store those in a vector database. \n",
    "\n",
    "That's what happens when we create the index. \n",
    "Now that we've got this index, we can use it during \n",
    "runtime to find the pieces of text most \n",
    "relevant to an incoming query. \n",
    "When a query comes in, we first create an \n",
    "embedding for that query. \n",
    "We then compare it to all the vectors \n",
    "in the vector database, and we pick the n most similar. \n",
    "These are then returned, and we can pass those in the prompt \n",
    "to the language model to get back a final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18013be",
   "metadata": {},
   "source": [
    "![Question](immagini/18_question.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c379f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=file)\n",
    "# We're going to create a document loader, loading from that CSV with all the descriptions of the \n",
    "# products that we want to do question answering over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc150538",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()\n",
    "# load documents from this document loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70248858",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0a815",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "```\n",
    "Document(page_content=\": 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\n\\nSize & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\n\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\n\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT® antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\n\\nQuestions? Please contact us for any inquiries.\", metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 0})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ec5f17",
   "metadata": {},
   "source": [
    "If we look at the individual documents, we can see that each \n",
    "document corresponds to one of the products in the CSV. \n",
    "Previously, we talked about creating chunks. \n",
    "Because these documents are already so small, \n",
    "we actually don't need to do any chunking here. \n",
    "And so we can create embeddings directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0a79ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba30096",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = embeddings.embed_query(\"Hi my name is Harrison\")\n",
    "# Let's use the \"embed_query\" method on the embeddings object to create an embeddings for a particular piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e200d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1175017a",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e547acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embed[:5])\n",
    "# this creates the overall numerical representation for this piece of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898aac2e",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "[-0.021913960576057434, 0.006774206645786762, -0.018190348520874977, -0.039148248732089996, -0.014089343138039112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd92ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs, \n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29420f89",
   "metadata": {},
   "source": [
    "We want to create embeddings for all the \n",
    "pieces of text that we just loaddand then \n",
    "we also want to store them in a vector store. \n",
    "We can do that by using the \"from_documents\" method \n",
    "on the vector store. \n",
    "This method takes in a list of documents, \n",
    "an embedding object, and then we'll create an overall vector store. We can now use this vector store to find pieces of text \n",
    "similar to an incoming query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89103bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Please suggest a shirt with sunblocking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc94c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff5311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed29d68f",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a383a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6485ce9b",
   "metadata": {},
   "source": [
    "*OUTPUT*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5437b984",
   "metadata": {},
   "source": [
    "```\n",
    "Document(page_content=': 255\\nname: Sun Shield Shirt by\\ndescription: \"Block the sun, not the fun – our high-performance sun shirt is guaranteed to protect from harmful UV rays. \\n\\nSize & Fit: Slightly Fitted: Softly shapes the body. Falls at hip.\\n\\nFabric & Care: 78% nylon, 22% Lycra Xtra Life fiber. UPF 50+ rated – the highest rated sun protection possible. Handwash, line dry.\\n\\nAdditional Features: Wicks moisture for quick-drying comfort. Fits comfortably over your favorite swimsuit. Abrasion resistant for season after season of wear. Imported.\\n\\nSun Protection That Won\\'t Wear Off\\nOur high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\'s harmful rays. This fabric is recommended by The Skin Cancer Foundation as an effective UV protectant.', metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 255})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208641e",
   "metadata": {},
   "source": [
    "## How do we use this to do question answering over our own documents?\n",
    "First, we need to create a \n",
    "retriever from this vector store. A retriever is a \n",
    "generic interface that can be underpinned by any \n",
    "method that takes in a query and returns documents.\n",
    "Vector stores and embeddings are one such method to do so, \n",
    "although there are plenty of different methods, \n",
    "some less advanced, some more advanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ba1b4",
   "metadata": {},
   "source": [
    "We want to do text generation and \n",
    "return a natural language response, we're going to import a \n",
    "language model and we're going to use ChatOpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ec39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cd9eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we were doing this by hand\n",
    "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f19af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We join all the page content in the documents into a variable \n",
    "# and then would pass this variable or a variant on the question into the language model. \n",
    "response = llm.call_as_llm(f\"{qdocs} Question: Please list all your \\\n",
    "shirts with sun protection in a table in markdown and summarize each one.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb881919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the respond\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b864eced",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "![Responde](immagini/21_responde.png)\n",
    "```\n",
    "All of these shirts provide UPF 50+ sun protection, blocking 98% of the sun's harmful rays. They also have additional features such as moisture- wicking, wrinkle-free fabric, and front/back cape venting for added comfort. The Sun Shield Shirt is recommended by The Skin Cancer Foundation for its effective UV protection.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c71724",
   "metadata": {},
   "source": [
    "The retriever we created above is just an \n",
    "interface for fetching documents. \n",
    "This will be used to fetch the documents and pass \n",
    "it to the language model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a499fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6c330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query =  \"Please list all your shirts with sun protection in a table \\\n",
    "in markdown and summarize each one.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c66e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa_stuff.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b575e85",
   "metadata": {},
   "source": [
    "![Responde](immagini/22_responde.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db2b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c3114",
   "metadata": {},
   "source": [
    "![Responde](immagini/23_responde.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cd07e2",
   "metadata": {},
   "source": [
    "#### Remember that we can still do it pretty easily with just the one line that we had up above. \n",
    "So, these two things equate to the same result. \n",
    "And that's part of the interesting stuff about LangChain. You \n",
    "can do it in one line, or you can look at \n",
    "the individual things and break it down into \n",
    "five more detailed ones. \n",
    "\n",
    "You \n",
    "can do it in one line, or you can look at \n",
    "the individual things and break it down into \n",
    "five more detailed ones. \n",
    "The five more detailed ones let you set \n",
    "more specifics about what exactly is going on, but the one-liner \n",
    "is easy to get started. So up to you as to how you'd prefer \n",
    "to go forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf49b488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize the index\n",
    "response = index.query(query, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4ce731",
   "metadata": {},
   "source": [
    "There's the same level of customization that you did when you create \n",
    "it by hand that's also available when you create the \n",
    "index here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c9500",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embeddings,\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6300472a",
   "metadata": {},
   "source": [
    "![Responde](immagini/24_responde.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41ca33d",
   "metadata": {},
   "source": [
    "![Responde](immagini/25_responde.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c69503",
   "metadata": {},
   "source": [
    "The stuff method is really nice because it's \n",
    "pretty simple. You just put all of it into one prompt and send that to \n",
    "the language model and get back one response. \n",
    "So it's quite simple to understand what's going \n",
    "on. It's quite cheap and it works pretty well. \n",
    "But that doesn't always work okay. \n",
    "So if you remember, when we fetched the \n",
    "documents in the notebook, we only got four documents back \n",
    "and they were relatively small. \n",
    "But what if you wanted to do the same type of question \n",
    "answering over lots of different types of chunks? \n",
    "\n",
    "Then there are a few different methods that we can use. \n",
    "The first is \"Map_reduce\". \n",
    "This basically takes all the chunks, passes them along with the \n",
    "question to a language model, gets back a response, and then uses \n",
    "another language model call to summarize all of the \n",
    "individual responses into a final answer. \n",
    "This is really powerful because it can operate \n",
    "over any number of documents. \n",
    "And it's also really powerful because you can do the \n",
    "individual questions in parallel. \n",
    "But it does take a lot more calls. And it does treat \n",
    "all the documents as independent, which may not always \n",
    "be the most desired thing. \"Refine\", which is another method, \n",
    "is again used to loop over many documents. \n",
    "But it actually does it iteratively. It builds upon the \n",
    "answer from the previous document. \n",
    "So this is really good for combining information and \n",
    "building up an answer over time. It will generally lead to longer \n",
    "answers. \n",
    "And it's also not as fast because now the calls aren't independent. \n",
    "They depend on the result of previous calls. \n",
    "This means that it often takes a good \n",
    "while longer and takes just as many calls as \"Map_reduce\", basically. \n",
    "\"Map_rerank\" is a pretty interesting and a bit more \n",
    "experimental one where you do a single call to the language model \n",
    "for each document. And you also ask it to return a score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e25fac",
   "metadata": {},
   "source": [
    "And then you select the highest score. \n",
    "This relies on the language model to know \n",
    "what the score should be. So you often have to tell it, \"Hey, \n",
    "it should be a high score if it's relevant to the document and really \n",
    "refine the instructions there\". Similar to \"Map_reduce\", all \n",
    "the calls are independent. So you \n",
    "can batch them and it's relatively fast. But again, you're making a bunch \n",
    "of language model calls. So it will be \n",
    "a bit more expensive. \n",
    "The most common of these methods is the \"stuff method\", \n",
    "which we used in the notebook to combine \n",
    "it all into one document. \n",
    "The second most common is the \"Map_reduce\" method, which takes these chunks \n",
    "and sends them to the language model. \n",
    "These methods here, stuff, map_reduce, refine, and rerank can also \n",
    "be used for lots of other chains besides just \n",
    "question answering. \n",
    "For example, a really common use case of the \"Map_reduce\" \n",
    "chain is for summarization, where you have a really long document \n",
    "and you want to recursively summarize \n",
    "pieces of information in it. \n",
    "That's it for question answering over documents. \n",
    "As you may have noticed, there's a lot going on in the \n",
    "different chains that we have here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
