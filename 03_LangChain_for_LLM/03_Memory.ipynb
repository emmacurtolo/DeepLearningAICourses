{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7a9e500",
   "metadata": {},
   "source": [
    "# LangChain: Memory\n",
    "\n",
    "## Outline\n",
    "* ConversationBufferMemory\n",
    "* ConversationBufferWindowMemory\n",
    "* ConversationTokenBufferMemory\n",
    "* ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5427b3ba",
   "metadata": {},
   "source": [
    "When you interact with these models, naturally they don't remember what you say before or any of the previous conversation, which is an issue when you're building some applications like chatbot and you want to have a conversation with them. And so in this section we'll cover memory, which is basically how do you remember previous parts of the conversation and feed that into the language model so that they can have this conversational flow as you're interacting with them.\n",
    "\n",
    "Langchain offers multiple sophisticated options for managing these memories. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf9c5c0",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b792bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138ba6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ff142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b29261",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6215a9",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "\"Hello Andrew! It's nice to meet you. How can I assist you today?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12756f6",
   "metadata": {},
   "source": [
    "If you want, you can change this verbose variable to True to see what Langchain is actually doing when you run predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842515cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da3733",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6ffe6",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "```\n",
    "> Entering new ConversationChain chain...\n",
    "Prompt after formatting:\n",
    "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "Human: Hi, my name is Andrew\n",
    "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
    "Human: What is 1+1?\n",
    "AI:\n",
    "\n",
    "> Finished chain.\n",
    "'1+1 is equal to 2.'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316cd523",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b285695",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "```\n",
    "> Entering new ConversationChain chain...\n",
    "Prompt after formatting:\n",
    "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "Human: Hi, my name is Andrew\n",
    "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
    "Human: What is 1+1?\n",
    "AI: 1+1 is equal to 2.\n",
    "Human: What is my name?\n",
    "AI:\n",
    "\n",
    "> Finished chain.\n",
    "'Your name is Andrew.'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d75ad9c",
   "metadata": {},
   "source": [
    "And notice that by the time I'm uttering the memory or the history of the conversation gets longer and longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa5cf86",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "```\n",
    "Human: Hi, my name is Andrew\n",
    "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
    "Human: What is 1+1?\n",
    "AI: 1+1 is equal to 2.\n",
    "Human: What is my name?\n",
    "AI: Your name is Andrew.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e5ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})\n",
    "# The curly braces here is actually an empty dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08db8c7",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "```\n",
    "{'history': \"Human: Hi, my name is Andrew\\nAI: Hello Andrew! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1+1 is equal to 2.\\nHuman: What is my name?\\nAI: Your name is Andrew.\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2886eff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab696b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d10cedc",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "```\n",
    "Human: Hi\n",
    "AI: What's up\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd0c1cc",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "```\n",
    "{'history': \"Human: Hi\\nAI: What's up\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b71b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2366a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f24aea",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "```\n",
    "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d7add1",
   "metadata": {},
   "source": [
    "When you use a large language model for a chat conversation, the launch language model itself is actually stateless. The language model itself does not remember the conversation you've had so far. And each transaction, each call to the API endpoint is independent. And chatbot is a period of memory only because there's usually wrapper code that provides the full conversation that's been had so far as context to the LM. And so the memory can store explicitly the turns or the utterances so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475ca981",
   "metadata": {},
   "source": [
    "![Memory](immagini/05_memory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baaea9c",
   "metadata": {},
   "source": [
    "The cost of sending a lot of tokens to the LM, which usually charges based on the number of tokens it needs to process, will also become more expensive. So Langchain provides several convenient kinds of memory to store and accumulate the conversation. Different type of memory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3d36e0",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9da508",
   "metadata": {},
   "source": [
    "I'm going to import the conversation buffer window memory that only keeps a window of memory. And if I set memory to conversational buffer window memory with __k equals one__, the variable k equals one specifies that I wanted to remember just one conversational exchange. That is, one utterance from me and one utterance from the chat bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5017926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7836ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba497d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3746cec",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "```\n",
    "{'history': 'Human: Not much, just hanging\\nAI: Cool'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42d673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ae532a",
   "metadata": {},
   "source": [
    "In practice, you probably won't use this with K equals one. You use this with K set to a larger number. But still this prevents the memory from growing without limit as the conversation goes longer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4002964",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be06544e",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "```\n",
    "\"Hello Andrew! It's nice to meet you. How can I assist you today?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f97bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617bca18",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "```\n",
    "'1+1 is equal to 2.'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e808b8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac70f808",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "```\n",
    "\"I'm sorry, but I don't have access to personal information.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85506622",
   "metadata": {},
   "source": [
    "With the conversational token buffer memory, the memory will limit the number of tokens saved. And because a lot of LM pricing is based on tokens, this maps more directly to the cost of the LM calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3ce445",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8a3d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae17afb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92d5827",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1388dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce195dca",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "```\n",
    "{'history': 'AI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4582e94e",
   "metadata": {},
   "source": [
    "Has the whole conversation signed with AI as what if I decrease it? Then it chops off the earlier parts of this conversation to retain the number of tokens corresponding to the most recent exchanges, but subject to not exceeding the token limit.\n",
    "\n",
    "llm=llm, this tells it to use the way of counting tokens that the chat OpenAI uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b70f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1935b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00e91ab",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "```\n",
    "{'history': 'Human: AI is what?!\\nAI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f3588",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03456e4e",
   "metadata": {},
   "source": [
    "The conversation summary buffer memory: the idea is, instead of limiting the memory to fixed number of tokens based on the most recent utterances or a fixed number of conversational exchanges. Let's use an llm to write a summary of the conversation so far and let that be the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f3d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc6678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=400)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29d5bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a941da0d",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "```\n",
    "{'history': \"Human: Hello\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: What is on the schedule today?\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6c10c9",
   "metadata": {},
   "source": [
    "Above --> Human & AI\n",
    "\n",
    "And the response is that long schedule.So this memory now has quite a lot of text in it. In fact, let's take a look at the memory variables .It contains that entire piece of text because 400 tokens was sufficient to store all this text. But now if I were to reduce the max token limit, say to 100 tokens, remember this stores the entire conversational history. If I reduce the number of tokens to 100, then the conversation summary buffer memory has actually used an LM, the OpenAI endpoint in this case, because that's what we have set the llm, to actually generate a summary of the conversation so far.\n",
    "\n",
    "Below --> System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a0af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e9dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b84f764",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "```\n",
    "{'history': 'System: The human and AI exchange greetings. The human asks about the schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting.'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afcc6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cefe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a5383",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "```\n",
    "> Entering new ConversationChain chain...\n",
    "Prompt after formatting:\n",
    "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "System: The human and AI exchange greetings. The human asks about the schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting.\n",
    "Human: What would be a good demo to show?\n",
    "AI:\n",
    "\n",
    "> Finished chain.\n",
    "\n",
    "'A good demo to show during the lunch meeting with the customer interested in AI would be the latest LLM (Language Model) demo. The LLM is a cutting-edge AI model that can generate human-like text based on a given prompt. It has been trained on a vast amount of data and can generate coherent and contextually relevant responses. By showcasing the LLM demo, you can demonstrate the capabilities of our AI technology and how it can be applied to various industries and use cases.'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7056c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def9abb6",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "```\n",
    "{'history': 'System: The human and AI exchange greetings and discuss the schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting. The human asks what would be a good demo to show, and the AI suggests showcasing the latest LLM (Language Model) demo. The LLM is a cutting-edge AI model that can generate human-like text based on a given prompt. It has been trained on a vast amount of data and can generate coherent and contextually relevant responses. By showcasing the LLM demo, the AI believes they can demonstrate the capabilities of their AI technology and how it can be applied to various industries and use cases.'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4037ee35",
   "metadata": {},
   "source": [
    "In the video, different types of memory used in Langchain were discussed. These memory types include:\n",
    "\n",
    "1. Buffer Memories: These memories have limitations based on the number of composition exchanges or tokens they can store. They can also summarize tokens above a certain limit.\n",
    "\n",
    "2. Vector Data Memory: This is a powerful memory type that stores word embeddings or text embeddings. It allows the retrieval of relevant blocks of text using these embeddings.\n",
    "\n",
    "3. Force Memory: The details of this memory type were not explained in the summary.\n",
    "\n",
    "4. Entity Memories: These memories are used to remember specific details about individuals or entities. For example, facts about a specific friend can be stored as an entity in an explicit way.\n",
    "\n",
    "When implementing applications using Langchain, developers can use multiple types of memories, combining conversation memory with entity memory. This allows the system to remember both a summary of the conversation and important facts about specific people involved.\n",
    "\n",
    "Additionally, developers may choose to store the entire conversation in a conventional database (such as a key-value store or SQL database) for auditing purposes or further system improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
