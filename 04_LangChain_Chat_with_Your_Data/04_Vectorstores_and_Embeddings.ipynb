{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "736b6f32",
   "metadata": {},
   "source": [
    "# Vectorstores and Embeddings\n",
    "\n",
    "Recall the overall workflow for retrieval augmented generation (RAG):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9557be0",
   "metadata": {},
   "source": [
    "![Vector](immagini/17_vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c99742",
   "metadata": {},
   "source": [
    "We've now got our document split up into small, semantically meaningful chunks, and it's time to put these chunks into an index, whereby we can easily retrieve them when it comes time to answer questions about this corpus of data. \n",
    "\n",
    "To do that, we're going to utilize __embeddings and vector stores__. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfc6996",
   "metadata": {},
   "source": [
    "First, these are incredibly important for building chatbots over your data. And second, we're going to go a bit deeper, and we're going to talk about edge cases, and where this generic method can actually fail. Don't worry, we're going to fix those later on. \n",
    "\n",
    "But for now, let's talk about vector stores and embeddings. \n",
    "\n",
    "And this comes after text splitting, when we're ready to store the documents in an easily accessible format. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985f9706",
   "metadata": {},
   "source": [
    "![Vector](immagini/18_vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf70ab75",
   "metadata": {},
   "source": [
    "What __EMBEDDINGS__ are? \n",
    "\n",
    "They take a piece of text, and they create a numerical representation of that text. Text with similar content will have similar vectors in this numeric space. What that means is we can then compare those vectors and find pieces of text that are similar. \n",
    "\n",
    "So, in the example below, we can see that the two sentences about pets are very similar, while a sentence about a pet and a sentence about a car are not very similar. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945076bc",
   "metadata": {},
   "source": [
    "![Vector](immagini/19_vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b656e0e",
   "metadata": {},
   "source": [
    "As a reminder of the full end-to-end workflow, we start with documents, we then create smaller splits of those documents, we then create embeddings of those documents, and then we store all of those in a vector store. \n",
    "\n",
    "A __VECTOR STORE__ is a database where you can easily look up similar vectors later on. This will become useful when we're trying to find documents that are relevant for a question at hand.\n",
    "\n",
    "We can then take the question at hand, create an embedding, and then do comparisons to all the different vectors in the vector store, and then pick the n most similar. We then take those n most similar chunks, and pass them along with the question into an LLM, and get back an answer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9fb109",
   "metadata": {},
   "source": [
    "![Vector](immagini/20_vector.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af22d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c107120",
   "metadata": {},
   "source": [
    "We just discussed `Document Loading` and `Splitting`.\n",
    "\n",
    "We're going to be working with the same set of documents. These are the CS229 lectures. \n",
    "\n",
    "Notice that we're actually going to duplicate the first lecture. This is for the purposes of simulating some dirty data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d30360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDF\n",
    "loaders = [\n",
    "    # Duplicate documents on purpose - messy data\n",
    "    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture02.pdf\"),\n",
    "    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture03.pdf\")\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc12d0",
   "metadata": {},
   "source": [
    "We can then use the recursive character text splitter to create chunks. We can see that we've now created over 200 different chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ecba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97724e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6cd3d4",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "209"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98f501",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Let's take our splits and embed them.\n",
    "\n",
    "Create embeddings for all of them. We'll use OpenAI to create these embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91386f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5fd6d6",
   "metadata": {},
   "source": [
    "The first two are very similar and the third one is unrelated.  We can then use the embedding class to create an embedding for each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106654ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"i like dogs\"\n",
    "sentence2 = \"i like canines\"\n",
    "sentence3 = \"the weather is ugly outside\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b067eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "embedding3 = embedding.embed_query(sentence3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a2fde",
   "metadata": {},
   "source": [
    "We can then use NumPy to compare them, and see which ones are most similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a761c8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61d9683",
   "metadata": {},
   "source": [
    "We'll use a dot product to compare the two embeddings.\n",
    "\n",
    "The important thing to know is that higher is better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3722c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4dfcc0",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "0.9631853877103519"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa565acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(embedding1, embedding3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a538f4",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "0.770999765129468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fea80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(embedding2, embedding3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f48aea",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "0.7596334120325541"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e746ac",
   "metadata": {},
   "source": [
    "## Vectorstores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c972af0a",
   "metadata": {},
   "source": [
    "### It's time to create embeddings for all the chunks of the PDFs and then store them in a vector store.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d453462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f578751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a199f5",
   "metadata": {},
   "source": [
    "The vector store that we'll use for this lesson is Chroma. So, let's import that. LangChain has integrations with lots, over 30 different vector stores. We choose Chroma because it's lightweight and in memory, which makes it very easy to get up and started with. There are other vector stores that offer hosted solutions, which can be useful when you're trying to persist large amounts of data or persist it in cloud storage somewhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df610bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in a variable called persist_directoty\n",
    "\n",
    "persist_directory = 'docs/chroma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209ff020",
   "metadata": {},
   "source": [
    "Let's also just make sure that nothing is there already. If there's stuff there already, it can throw things off and we don't want that to happen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b09c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./docs/chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892fed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now create the vector store\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory  # allows us to save the directory to disk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27088db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f887332",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "209\n",
    "\n",
    "We can see that it's 209, which is the same as the number of splits that we had from before. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f378c6",
   "metadata": {},
   "source": [
    "### Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b48c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"is there an email i can ask for help\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ab3ee4",
   "metadata": {},
   "source": [
    "We're going to use the similarity_search method, and we're going to pass in the question, and then we'll also pass in k=3. This specifies the number of documents that we want to return. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8755c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectordb.similarity_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db7b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219f3147",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e58cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11efec40",
   "metadata": {},
   "source": [
    "*OUTPUT*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6454cf9",
   "metadata": {},
   "source": [
    "![Vector](immagini/21_vector.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd550b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a77c8050",
   "metadata": {},
   "source": [
    "Let's save this so we can use it later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de9983",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ec369c",
   "metadata": {},
   "source": [
    "## Failure modes\n",
    "\n",
    "This seems great, and basic similarity search will get you 80% of the way there very easily. \n",
    "\n",
    "But there are some failure modes that can creep up. \n",
    "\n",
    "Here are some edge cases that can arise - we'll fix them in the next class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1840543",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what did they say about matlab?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9861ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectordb.similarity_search(question,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387396e1",
   "metadata": {},
   "source": [
    "Notice that we're getting duplicate chunks (because of the duplicate `MachineLearning-Lecture01.pdf` in the index).\n",
    "\n",
    "Semantic search fetches all similar documents, but does not enforce diversity.\n",
    "\n",
    "`docs[0]` and `docs[1]` are indentical.\n",
    "\n",
    "If we take a look at the first two results, we can see that they're actually identical. This is because when we loaded in the PDFs, if you remember, we specified on purpose a duplicate entry. \n",
    "This is bad because we've got the same information in two different chunks and we're going to be passing both of these chunks to the language model down the line. There's no real value in the second piece of information and it would be much better if there was a different distinct chunk that the language model could learn from. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b461d",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "![Vector](immagini/22_vector.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35d3bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf1e1b7",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "![Vector](immagini/22_vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca17a05",
   "metadata": {},
   "source": [
    "We can see a new failure mode.\n",
    "\n",
    "The question below asks a question about the third lecture, but includes results from other lectures as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a1493",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what did they say about regression in the third lecture?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c3c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectordb.similarity_search(question,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549512ed",
   "metadata": {},
   "source": [
    "So, let's loop over all the documents and print out the metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c3d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4daa846",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "![Vector](immagini/23_vector.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad4a664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f93ae639",
   "metadata": {},
   "source": [
    "We can see that there's actually a combination of results, some from the third lecture, some from the second lecture, and some from the first. The intuition about why this is failing is that the third lecture and the fact that we want documents from only the third lecture is a piece of structured information, but we're just doing a semantic lookup based on embeddings where it creates an embedding for the whole sentence and it's probably a bit more focused on regression. Therefore, we're getting results that are probably pretty relevant to regression and so if we take a look at the fifth doc, the one that comes from the first lecture, we can see that it does in fact mention regression. So, it's picking up on that, but it's not picking up on the fact that it's only supposed to be querying documents from the third lecture because again, that's a piece of structured information that isn't really perfectly captured in this semantic embedding that we've created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fcaec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[4].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a951fb6c",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "![Vector](immagini/24_vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b9f28c",
   "metadata": {},
   "source": [
    "You'll probably notice that when you make it larger, you'll retrieve more documents, but the documents towards the tail end of that may not be as relevant as the ones at the beginning. \n",
    "\n",
    "Approaches discussed in the next lecture can be used to address both!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
