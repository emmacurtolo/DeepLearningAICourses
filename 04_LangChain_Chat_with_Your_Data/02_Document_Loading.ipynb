{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d5e617d",
   "metadata": {},
   "source": [
    "# Document Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd1022f",
   "metadata": {},
   "source": [
    "In order to create an application where you \n",
    "can chat with your data, you first have to load \n",
    "your data into a format where it can be worked with. That's \n",
    "where LangChain document loaders come into play. We \n",
    "have over 80 different types of document loaders, and \n",
    "in this lesson we'll cover a few of the \n",
    "most important ones and get you comfortable with \n",
    "the concept in general. \n",
    "\n",
    "Let's jump in! \n",
    "\n",
    "Document loaders deal with the specifics of accessing and \n",
    "converting data from a variety of different formats and \n",
    "sources into a standardized format. There can be different \n",
    "places that we want to load data from, like websites, \n",
    "different databases, YouTube, and these documents can \n",
    "come in different data types, like PDFs, HTML, JSON. \n",
    "\n",
    "__And so the whole purpose of document loaders \n",
    "is to take this variety of data sources \n",
    "and load them into a standard document object. \n",
    "Which consists of content and then associated metadata.__\n",
    "\n",
    "There are a lot of different type of \n",
    "document loaders in LangChain, and we won't \n",
    "have time to cover them all, but here is a rough \n",
    "categorization of the 80 plus that we have. There \n",
    "are a lot that deal with loading __unstructured data__, like \n",
    "text files, from public data sources, like \n",
    "YouTube, Twitter, Hacker News, and there are also even more that \n",
    "deal with loading unstructured data from \n",
    "the proprietary data sources that you or your company \n",
    "might have, like Figma, Notion. \n",
    "\n",
    "Document loaders can also be used to load __structured data__. \n",
    "Data that's in a tabular format and may just have \n",
    "some text data in one of those cells or rows that you \n",
    "still want to do question answering or semantic \n",
    "search over. And so the sources here \n",
    "include things like Airbyte, Stripe, Airtable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413b11a5",
   "metadata": {},
   "source": [
    "![Image](immagini/05_document.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e0a51",
   "metadata": {},
   "source": [
    "![Image](immagini/06_document.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0b1d24",
   "metadata": {},
   "source": [
    "## Note to students.\n",
    "During periods of high load you may find the notebook unresponsive. It may appear to execute a cell, update the completion number in brackets [#] at the left of the cell but you may find the cell has not executed. This is particularly obvious on print statements when there is no output. If this happens, restart the kernel using the command under the Kernel tab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f163429",
   "metadata": {},
   "source": [
    "## Retrieval augmented generation\n",
    " \n",
    "In retrieval augmented generation (RAG), an LLM retrieves contextual documents from an external dataset as part of its execution. \n",
    "\n",
    "This is useful if we want to ask question about specific documents (e.g., our PDFs, a set of videos, etc). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee7d1b",
   "metadata": {},
   "source": [
    "![Image](immagini/07_document.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed3c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2766df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155c9395",
   "metadata": {},
   "source": [
    "## PDFs\n",
    "\n",
    "Let's load a PDF [transcript](https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture01.pdf) from Andrew Ng's famous CS229 course! These documents are the result of automated transcription so words and sentences are sometimes split unexpectedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee3d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The course will show the pip installs you would need to install packages on your own machine.\n",
    "# These packages are already installed on this platform and should not be run again.\n",
    "#! pip install pypdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the relevant document loader from Langchain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# workspace\n",
    "loader = PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\")\n",
    "\n",
    "# load the document with the load function\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a33a753",
   "metadata": {},
   "source": [
    "Let's have a look of what we just loaded:\n",
    "\n",
    "Each page is a `Document`.\n",
    "\n",
    "A `Document` contains text (`page_content`) and `metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674feef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of documents\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b570312a",
   "metadata": {},
   "source": [
    "*OUTPUT*\n",
    "\n",
    "22\n",
    "\n",
    "there are 22 different pages in this PDF. \n",
    "Each one is its own unique document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d7f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the first document (page_content)\n",
    "\n",
    "page = pages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e399d66",
   "metadata": {},
   "source": [
    "## .page_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a66d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the first few hundred characters\n",
    "\n",
    "print(page.page_content[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a99ce0b",
   "metadata": {},
   "source": [
    "*OUTPUT*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a929d594",
   "metadata": {},
   "source": [
    "```\n",
    "MachineLearning-Lecture01  \n",
    "Instructor (Andrew Ng):  Okay. Good morning. Welcome to CS229, the machine \n",
    "learning class. So what I wanna do today is ju st spend a little time going over the logistics \n",
    "of the class, and then we'll start to  talk a bit about machine learning.  \n",
    "By way of introduction, my name's  Andrew Ng and I'll be instru ctor for this class. And so \n",
    "I personally work in machine learning, and I' ve worked on it for about 15 years now, and \n",
    "I actually think that machine learning i\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d452f3ec",
   "metadata": {},
   "source": [
    "## .metadata\n",
    "\n",
    "The first thing the document consists of is some page content, which is the content of the page. This can be a bit long, so let's just print out the first few hundred characters. \n",
    "The other piece of information that's really important is the metadata associated with each document. This can be accessed with the metadata element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6046a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02642a38",
   "metadata": {},
   "source": [
    "*OUTPUT*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc504c50",
   "metadata": {},
   "source": [
    "```\n",
    "{'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b76d55",
   "metadata": {},
   "source": [
    "You can see here that there's two different pieces. \n",
    "- One is the source information. This is the PDF, the name of the file that we loaded it from. \n",
    "- The other is the page field. This corresponds to the page of the PDF that it was loaded from. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d307741f",
   "metadata": {},
   "source": [
    "## YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07888046",
   "metadata": {},
   "source": [
    "We're going to import a few different things here. The key parts are the YouTube audio loader, which loads an audio file from a YouTube video. The other key part is the OpenAI Whisper parser. This will use OpenAI's Whisper model, a speech-to-text model, to convert the YouTube audio into a text format that we can work with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febda762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
    "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40228b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install yt_dlp\n",
    "# ! pip install pydub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f0598c",
   "metadata": {},
   "source": [
    "**Note**: This can take several minutes to complete.\n",
    "\n",
    "We can now specify a URL, specify a directory in which to save the audio files, and then create the generic loader as a combination of this YouTube audio loader combined with the OpenAI Whisper parser. And then we can call \"loader.load\" to load the documents corresponding to this YouTube. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc88dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.youtube.com/watch?v=jGwO_UgTS7I\"\n",
    "save_dir=\"docs/youtube/\"\n",
    "loader = GenericLoader(\n",
    "    YoutubeAudioLoader([url],save_dir),\n",
    "    OpenAIWhisperParser()\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da64400f",
   "metadata": {},
   "source": [
    "![Image](immagini/09_document.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1471446",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e3a16",
   "metadata": {},
   "source": [
    "![Image](immagini/10_document.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b177455c",
   "metadata": {},
   "source": [
    "## URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66d287",
   "metadata": {},
   "source": [
    "The next set of documents that we're going to go over how to load are URLs from the Internet. There's a lot of really awesome educational content on the Internet, and wouldn't it be cool if you could just chat with it? \n",
    "\n",
    "We're going to enable that by importing the web-based loader from LangChain. Then we can choose any URL, our favorite URL. Here, we're going to choose a markdown file from this GitHub page and create a loader for it. And then next, we can call loader.load, and then we can take a look at the content of the page. \n",
    "\n",
    "Here, you'll notice there's a lot of white space, followed by some initial text, and then some more text. This is a good example of why you actually need to do some post-processing on the information to get it into a workable format.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5099a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://github.com/basecamp/handbook/blob/master/37signals-is-you.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f944c101",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550fc785",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b069f6",
   "metadata": {},
   "source": [
    "![Image](immagini/11_document.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced829ed",
   "metadata": {},
   "source": [
    "## Notion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd19b58",
   "metadata": {},
   "source": [
    "Follow steps [here](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/notion) for an example Notion site such as [this one](https://yolospace.notion.site/Blendle-s-Employee-Handbook-e31bff7da17346ee99f531087d8b133f):\n",
    "\n",
    "* Duplicate the page into your own Notion space and export as `Markdown / CSV`.\n",
    "* Unzip it and save it as a folder that contains the markdown file for the Notion page.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a7eb4b",
   "metadata": {},
   "source": [
    "![Image](immagini/08_document.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357a655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "loader = NotionDirectoryLoader(\"docs/Notion_DB\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e2f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content[0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8879589",
   "metadata": {},
   "source": [
    "![Image](immagini/12_document.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9e9b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b2a43",
   "metadata": {},
   "source": [
    "Finally, we'll cover how to load data from Notion. Notion is a really popular store of both personal and company data, and a lot of people have created chatbots talking to their Notion databases. In your notebook, you'll see instructions on how to export data from your Notion database into a format through which we can load it into LangChain. Once we have it in that format, we can use the Notion directory loader to load that data and get documents that we can work with. If we take a look at the content here, we can see that it's in markdown format, and this Notion document is from Blendle's Employee Handbook. \n",
    "\n",
    "I'm sure a lot of people listening have used Notion and have some Notion databases that they would like to chat with, and so this is a great opportunity to go export that data, bring it in here, and start working with it in this format. That's it for document loading. Here, we've covered how to load data from a variety of sources and get it into a standardized document interface. \n",
    "\n",
    "However, these documents are still rather large, and so in the next section, we're going to go over how to split them up into smaller chunks. \n",
    "\n",
    "This is relevant and important because when you're doing this retrieval augmented generation, you need to retrieve only the pieces of content that are most relevant, and so you don't want to select the whole documents that we've loaded here, but rather only the paragraph or few sentences that are most topical to what you're talking about.  This is also an even better opportunity to think about what sources of data we don't currently have loaders for, but you might still want to explore. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a861f95a",
   "metadata": {},
   "source": [
    "![Image](immagini/13_document.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
